{
 "metadata": {
  "name": "",
  "signature": "sha256:3595f1e57b069ad47414fc291224c6c0d7905bc5077008e631bb440e8ee9efcb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/mindhive/saxelab/scripts/aesscripts/')\n",
      "import pickle\n",
      "import NDE as ndef\n",
      "import NDIM as ndimf\n",
      "import os\n",
      "import pandas as pd\n",
      "import FGE_MISC.code.behave.stimanalysisfuncs as saf\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import FGE_MISC.code.vizfuncs as vizfuncs\n",
      "from FGE_MISC.code.config import cohmetrixcfg, rootdir, cfg, dfiles #all hardcoding of paths, etc. lives in here\n",
      "from joblib import Parallel, delayed  \n",
      "import multiprocessing\n",
      "num_cores = multiprocessing.cpu_count()\n",
      "print \"%s cores available for parallel processing\" % num_cores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "64 cores available for parallel processing\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeling=False #are you redoing the modeling or loading most recent?\n",
      "simspacing=False #are you reconstructing the similarity spaces or loading most recent?\n",
      "createdfs=False #are you creating the initial input spaces (e.g. training on rotten tomatoes) or loading existing\n",
      "#cfg.printdeets()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preprocessing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#analyze NDE\n",
      "%autoreload\n",
      "NDE_output, orderedemos, ndedf=ndef.main(dfiles.ndefile, cfg)\n",
      "ndimf.quicksave(NDE_output, os.path.join(rootdir, 'results', 'NDE_output.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running setup for NDE...\n",
        "170 subjects, 94 females"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mean age: 33.382, sem: 0.880\n",
        "starting with 190 rows\n",
        "reduced to 170 intact rows\n",
        "reduced to 137 check-passing rows\n",
        "reduced from 820 to 408 columns"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "item-wise results:\n",
        "overall accuracy: 0.649%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "one-way anova comparing classification accuracy across emotions\n",
        "F(19,180)=4.90, p=0.000000\n",
        "independent samples ttest comparing classification accuracy across positive and negative emotions\n",
        "Mneg(SEM)=0.69(0.02), Mpos(SEM)=0.59(0.03), t(198)=3.02, p=0.002920\n",
        "\n",
        "collapsing over items:\n",
        "NDE analysis complete."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load stimuli\n",
      "stimdf=pd.read_csv(dfiles.stimfile)\n",
      "item2emomapping=ndimf.createitem2emomapping(stimdf)\n",
      "#load appraisals\n",
      "appraisalsdf,appraisalscdf=pd.read_csv(dfiles.appraisalfile),pd.read_csv(dfiles.appraisalcfile)\n",
      "appraisalsdf.ix[appraisalsdf[appraisalsdf['Dqname']=='safey'].index,'Dqname']='safety' #fix typo\n",
      "appraisals=[el for el in appraisalsdf['Dqname'].values if el not in cfg.ndimchecks.keys() and el !='emotion']\n",
      "appraisalsc=[el for el in appraisalscdf['Dqname'].values if el not in cfg.ndimchecks.keys() and el !='emotion']+['disgusted_dim', 'surprised_dim'] #hack :["
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load and preprocess data\n",
      "%autoreload\n",
      "print \"working on NT exp\"\n",
      "ndimdf, ndimsubjdf, ndimexplicitdf=ndimf.setup(dfiles.ndimfile, cfg.ndimchecks, item2emomapping, orderedemos, appraisals, cfg.subjcols, cfg.fixedcols, visualize=False) \n",
      "ndimdf.ix[1701,'altering']=5 #hack to fix an acquisition error\n",
      "print \"working on NT conrol\"\n",
      "ndimcdf, ndimcsubjdf, ndimcexplicitdf=ndimf.setup(dfiles.ndimcfile, cfg.ndimchecks, item2emomapping, orderedemos, appraisalsc, cfg.subjcols, cfg.fixedcols,visualize=False) \n",
      "dfdict=ndimf.splitdf(ndimcdf, cfg.appraisalsubsets, cfg.fixedcols) #split into basicemo and valencearousal\n",
      "ndimbedf,ndimvadf,ndimvdf,ndimadf=dfdict['be'],dfdict['va'],dfdict['v'],dfdict['a']\n",
      "print \"total specs across NT exp and control\"\n",
      "tsubjdf=ndimf.summarize(ndimdf, ndimsubjdf, ndimcdf, ndimcsubjdf)\n",
      "print \"working on ASD exp\"\n",
      "ndimasddf, ndimasdsubjdf, ndimasdexplicitdf=ndimf.setup(dfiles.ndimasdfile, cfg.asdchecks, item2emomapping, orderedemos, appraisals, cfg.subjcols, cfg.fixedcols,visualize=False) \n",
      "print \"combining explicits\"\n",
      "allexplicitsdf=pd.concat([ndimcexplicitdf,ndimexplicitdf])\n",
      "allexplicitsdf.index=range(len(allexplicitsdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "working on NT exp\n",
        "starting with 1971 rows...."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced to 1864 items by removing all-null entries...."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "altered out of range values for 1 datapoints....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced to 1552 items by removing check-failing subjects (avgthresh=7)....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced to 1520 check-passing items (indthresh=5).....\n",
        "reduced to 1520 after eliminating acquisition errors......"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "added item and emo column"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced from 160 to 63 columns"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data from 189 unique subjects\n",
        "6 to 11 responses per item (mean=7.600, sem=0.064)\n",
        "working on NT conrol"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "starting with 1600 rows....\n",
        "hack to convert disgusted and surprised from emos to appraisals\n",
        "reduced to 1581 items by removing all-null entries...."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "altered out of range values for 0 datapoints....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced to 1565 items by removing check-failing subjects (avgthresh=7)....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced to 1556 check-passing items (indthresh=5).....\n",
        "reduced to 1556 after eliminating acquisition errors......"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "added item and emo column"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced from 52 to 33 columns\n",
        "data from 39 unique subjects\n",
        "6 to 11 responses per item (mean=7.780, sem=0.093)\n",
        "split dataframe into 4 dataframes: a, be, v, va\n",
        "total specs across NT exp and control\n",
        "3076 individual item responses, 228 subjects\n",
        "108 females, age: mean(sem)=34.47(0.77)\n",
        "13 to 20 responses per item (mean=15.380, sem=0.114)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "working on ASD exp\n",
        "starting with 238 rows....\n",
        "reduced to 227 items by removing all-null entries...."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "altered out of range values for 0 datapoints....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced to 227 items by removing check-failing subjects (avgthresh=0).....\n",
        "reduced to 201 check-passing items (indthresh=0).....\n",
        "reduced to 201 after eliminating acquisition errors......"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "added item and emo column"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reduced from 160 to 63 columns"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data from 9 unique subjects\n",
        "7 to 12 responses per item (mean=10.050, sem=0.296)\n",
        "combining explicits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "NDIM.py:142: UserWarning:\n",
        "\n",
        "value out of range in row 1701: column altering has an out of range value 510.0\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dimensionality reduction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avgs=ndimdf.groupby('item')[appraisals].mean()\n",
      "dimlabels,dimmatrix,axislabels=avgs.columns, avgs.values.T, avgs.index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#PCA\n",
      "%autoreload\n",
      "plotit=False\n",
      "pcaobsscores, pcadimloadings, pcarecoveredinput, pcavarexplained=ndimf.reduction(dimmatrix.T, reductiontype='PCA', plotit=plotit)\n",
      "pcaresults={'obsscores':pcaobsscores, 'dimloadings':pcadimloadings, 'inputmatrix':dimmatrix.T, 'recoveredinput':pcarecoveredinput, 'varexplained':pcavarexplained, 'stimlabels':axislabels.values, 'dimlabels':dimlabels.values}\n",
      "ndimf.quicksave(pcaresults, os.path.join(rootdir,'results','PCARESULTS_all.pkl'))\n",
      "pcaobsscores, pcadimloadings, pcarecoveredinput, pcavarexplained=ndimf.reduction(dimmatrix.T, ncomp=3, reductiontype='PCA', plotit=plotit)#set this based on first screen plot\n",
      "pcaresults={'obsscores':pcaobsscores, 'dimloadings':pcadimloadings, 'inputmatrix':dimmatrix.T, 'recoveredinput':pcarecoveredinput, 'varexplained':pcavarexplained, 'stimlabels':axislabels.values, 'dimlabels':dimlabels.values}\n",
      "ndimf.quicksave(pcaresults, os.path.join(rootdir,'results','PCARESULTS_reduced.pkl'))\n",
      "if plotit:\n",
      "    vizfuncs.plotinputincomponentspace(pcaobsscores, axislabels, item2emomapping, plotlabels=False)\n",
      "    vizfuncs.plotdimensionsincomponentspace(pcadimloadings, dimlabels, mapping=None, componentindices=[0, 1, 2], title=None)\n",
      "ndimf.printhighlowloaders(pcadimloadings, pcavarexplained, dimlabels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reduced to 38 dimensions\n",
        "explaining 100.000% of variance\n",
        "reduced to 3 dimensions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "explaining 57.845% of variance\n",
        "component 0 (explaining 34.47% of variance):\n",
        "    high loaders= pleasantness, goal_consistency, fairness (0.4,0.3,0.3)\n",
        "component 1 (explaining 12.97% of variance):\n",
        "    high loaders= people, relationship_influence, close_others (0.4,0.4,0.3)\n",
        "component 2 (explaining 10.41% of variance):\n",
        "    high loaders= self_cause, control, safety (0.4,0.3,0.3)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "iterativeresults=ndimf.iterativeregression(avgs, rootdir,plotit=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pleasantness, relationship_influence, self_cause, relevance, bodily_disease, agent_cause, repetition, occurred, people, past, suddenness, self_involvement, close_others, others_knowledge, agent_intention, danger, knowledge_change, consequences, altering, certainty, attention, freedom, moral, familiarity, future, mental_states, agent_situation, psychological_change, safety, remember, pressure, selfesteem, control, expectedness, self_consistency, fairness, coping, goal_consistency\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Stimulus Analyses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "#load raw stim and other models/feature spaces\n",
      "if createdfs:\n",
      "    FGEstimdf=pd.read_csv(dfiles.stimfile, index_col=False)\n",
      "    cohmetrixinputdf=pd.read_csv(dfiles.cohmetrixfile, index_col=False)\n",
      "    RNTNinputdf=pd.read_csv(dfiles.sentimentfile, index_col=False) #generated by analyzeFGEstimsentiment.py\n",
      "    intensityinputdf=pd.read_csv(dfiles.intensityfile, index_col=False)\n",
      "    # compute spaces\n",
      "    indfeaturespaces=saf.computeindfeatspace(avgs, item2emomapping)\n",
      "    best10featuresdf=saf.computebestfeatspace(avgs, iterativeresults, item2emomapping, numdims=10)\n",
      "    [pca0df,pca1df,pca2df]=saf.computePCAspace(pcaresults, item2emomapping) #match to number of PCs you are extracting above\n",
      "    NDEconfdf=saf.computeNDEspace(NDE_output, item2emomapping)\n",
      "    cohmetrixdf = saf.computecohmetrixspace(cohmetrixinputdf, item2emomapping, cols=cohmetrixcfg.cols, excludecols=cohmetrixcfg.excludecols, filename='cohmetrixdf')\n",
      "    syntaxdf = saf.computecohmetrixspace(cohmetrixinputdf, item2emomapping, cols=cohmetrixcfg.syntaxcols, excludecols=cohmetrixcfg.excludecols, filename='syntaxdf')\n",
      "    easedf = saf.computecohmetrixspace(cohmetrixinputdf, item2emomapping, cols=cohmetrixcfg.easecols, excludecols=cohmetrixcfg.excludecols, filename='easedf')\n",
      "    bagofwordsdf =saf.bagofwords(FGEstimdf, item2emomapping)\n",
      "    cosinedict, tfidfdf = saf.tfidf(FGEstimdf,item2emomapping, visualize=False)\n",
      "    sentbowdf = saf.sentimentbow(FGEstimdf,item2emomapping)\n",
      "    intensitydf = saf.behavioralintensity(intensityinputdf, item2emomapping)\n",
      "    sentrntndf = saf.sentimentrntn(RNTNinputdf, item2emomapping)\n",
      "else:\n",
      "    dfnames=['NDEconfdf','best10featuresdf','cohmetrixdf','syntaxdf','easedf','tfidfdf','sentbowdf','intensitydf','sentrntndf','bagofwordsdf']\n",
      "    [NDEconfdf,best10featuresdf,cohmetrixdf,syntaxdf,easedf,tfidfdf,sentbowdf,intensitydf,sentrntndf, bagofwordsdf]=[ndimf.quickload(os.path.join(rootdir,'data/stimdfs','%s.pkl' %fname)) for fname in dfnames]\n",
      "    [pca0df,pca1df,pca2df]=[ndimf.quickload(os.path.join(rootdir,'data/stimdfs','%s.pkl' %fname)) for fname in ['pcacomp0','pcacomp1','pcacomp2']]\n",
      "    indfeaturespaces=ndimf.quickload(os.path.join(rootdir,'data/stimdfs','indfeaturedfs.pkl'))\n",
      "    print \"loaded feature spaces\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded feature spaces\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Modeling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allresults={} #save all results to this dict\n",
      "confmatsaves={}\n",
      "cfg.fixedcols.append('CVI')\n",
      "dfdict={'behavioralconfs':NDEconfdf,'best10features':best10featuresdf, '1stPC':pca0df,'2ndPC':pca1df,'3rdPC':pca2df,'cohmetrix':cohmetrixdf,\n",
      "        'syntax':syntaxdf,'readingease':easedf,'tfidf':tfidfdf,'sentimentBoW':sentbowdf,\n",
      "        'intensity':intensitydf,'sentimentRNTN':sentrntndf,'bagofwords':bagofwordsdf}\n",
      "for key in indfeaturespaces.keys():\n",
      "    dfdict[key]=indfeaturespaces[key]\n",
      "inputs={'labelcol':'emo', 'iterations':1000, 'folds':{'train':[0], 'test':[1], 'nfolds':2}, 'comparisons':NDE_output} #alternative: comparisons-->NDE_output\n",
      "modeltypes={'37appraisals': [ndimdf,appraisals],'37dimASD': [ndimasddf,appraisals],'basicemo': [ndimbedf,cfg.appraisalsubsets['be']],'valencearousal': [ndimvadf,cfg.appraisalsubsets['va']],'valence': [ndimvdf,cfg.appraisalsubsets['v']],'arousal': [ndimadf,cfg.appraisalsubsets['a']]}\n",
      "modeltypes['explicits']=[allexplicitsdf, [col for col in allexplicitsdf.columns if 'extent' in col]]\n",
      "for key in dfdict.keys():\n",
      "    modeltypes[key]=[dfdict[key], [col for col in dfdict[key].columns if col not in cfg.fixedcols]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parresults = Parallel(n_jobs=num_cores)(delayed(ndimf.classpar)(chosenmodel, modeltypes, allresults, inputs, orderedemos, rootdir) for chosenmodel in modelnames)  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for chosenmodeln, chosenmodel in enumerate(modelnames):\n",
      "    allresults['%s_result_%s' % (chosenmodel, 'None')] = parresults[chosenmodeln][0]\n",
      "    allresults['%s_result_%s' % (chosenmodel, 'item')] = parresults[chosenmodeln][1]\n",
      "    confmatsaves['%s_confs_item' %(chosenmodel)] = parresults[chosenmodeln][2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#modeling parallel\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "if modeling:\n",
      "    modelnames=[m for m in modeltypes.keys() if m !='37dimASD']\n",
      "    parresults = Parallel(n_jobs=num_cores)(delayed(ndimf.classpar)(chosenmodel, modeltypes, allresults, inputs, orderedemos, rootdir) for chosenmodel in modelnames)  \n",
      "    for chosenmodeln, chosenmodel in enumerate(modelnames):\n",
      "        allresults['%s_result_%s' % (chosenmodel, 'None')] = parresults[chosenmodeln][0]\n",
      "        allresults['%s_result_%s' % (chosenmodel, 'item')] = parresults[chosenmodeln][1]\n",
      "        confmatsaves['%s_confs_item' %(chosenmodel)] = parresults[chosenmodeln][2]\n",
      "    ndimf.quicksave(confmatsaves, os.path.join(rootdir,'similarityspaces','NDIMsvmerrors.pkl'))\n",
      "    ndimf.quicksave(allresults, os.path.join(rootdir,'results','NDIMRESULTS.pkl'))\n",
      "else:\n",
      "    confmatsaves=ndimf.quickload(os.path.join(rootdir,'similarityspaces','NDIMsvmerrors.pkl'))\n",
      "    allresults=ndimf.quickload(os.path.join(rootdir,'results','NDIMRESULTS.pkl'))\n",
      "    print \"loaded allresults and confmatsaves\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded allresults and confmatsaves\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "m2bresults={}\n",
      "for k in allresults.keys():\n",
      "    m2bresults[k]=ndimf.comparemodel2behavior(allresults[k], NDE_output, orderedemos, simtype='pearsonr')\n",
      "ndimf.quicksave(m2bresults, os.path.join(rootdir,'results','M2BRESULTS.pkl'))\n",
      "print \"correlated model outputs and behavior\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "correlated model outputs and behavior\n",
        "overall classification across iterations: 11.87%overall classification across iterations: 9.69%overall classification across iterations: 9.45%overall classification across iterations: 49.26%overall classification across iterations: 9.60%overall classification across iterations: 15.75%overall classification across iterations: 15.53%overall classification across iterations: 11.26%overall classification across iterations: 13.23%overall classification across iterations: 45.43%overall classification across iterations: 12.89%overall classification across iterations: 13.33%overall classification across iterations: 9.45%overall classification across iterations: 20.22%overall classification across iterations: 11.25%overall classification across iterations: 5.74%overall classification across iterations: 8.15%overall classification across iterations: 14.09%overall classification across iterations: 10.29%overall classification across iterations: 15.72%overall classification across iterations: 13.20%overall classification across iterations: 9.70%overall classification across iterations: 16.78%overall classification across iterations: 15.19%overall classification across iterations: 7.80%overall classification across iterations: 10.25%overall classification across iterations: 9.23%overall classification across iterations: 12.02%overall classification across iterations: 12.29%overall classification across iterations: 12.61%overall classification across iterations: 11.61%overall classification across iterations: 56.58%overall classification across iterations: 19.31%overall classification across iterations: 8.95%overall classification across iterations: 10.02%overall classification across iterations: 87.73%\n",
        "\n",
        "\n",
        "\n",
        "overall classification across iterations: 7.33%\n",
        "overall classification across iterations: 11.70%\n",
        "overall classification across iterations: 16.87%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autoreload\n",
      "#in progress individual analysis\n",
      "indresults={}\n",
      "for chosenmodel in ['37appraisals', 'basicemo', 'valencearousal', 'valence', 'arousal']:\n",
      "    print chosenmodel\n",
      "    modeldata,features=modeltypes[chosenmodel][0],modeltypes[chosenmodel][1]\n",
      "    indresults[chosenmodel]=ndimf.individualizedmodeling(modeldata, ndimexplicitdf, chosenmodel, inputs['labelcol'], features, 'item', inputs['iterations'], orderedemos, inputs['folds'], rootdir)\n",
      "ndimf.quicksave(indresults, os.path.join(rootdir,'results','INDRESULTS.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37appraisals\n",
        "training on 1042 pass items, testing on 292 failed items:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy (relative to intended emotion): 0.342, user-consistency (prediction aligns with user max): 0.123\n",
        "basicemo\n",
        "training on 765 pass items, testing on 233 failed items:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy (relative to intended emotion): 0.270, user-consistency (prediction aligns with user max): 0.069\n",
        "valencearousal\n",
        "training on 765 pass items, testing on 233 failed items:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy (relative to intended emotion): 0.180, user-consistency (prediction aligns with user max): 0.056\n",
        "valence\n",
        "training on 765 pass items, testing on 233 failed items:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy (relative to intended emotion): 0.120, user-consistency (prediction aligns with user max): 0.052\n",
        "arousal\n",
        "training on 765 pass items, testing on 233 failed items:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy (relative to intended emotion): 0.133, user-consistency (prediction aligns with user max): 0.039\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Similarity Spaces"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#save input feature spaces and rsa similarity spaces\n",
      "%autoreload\n",
      "if simspacing:\n",
      "    inputspaces,allrsasimspaces={},{}\n",
      "    rdmiterations= inputs['iterations']\n",
      "    for chosenmodel in [m for m in modeltypes.keys() if m !='37dimASD']:\n",
      "        df=modeltypes[chosenmodel][0]\n",
      "        features=['item']+modeltypes[chosenmodel][1]\n",
      "        rsadict=ndimf.createfeaturespace(df, features) #model inputspace\n",
      "        inputspaces[chosenmodel]=rsadict\n",
      "        allrsasimspaces['%s_rdm' %chosenmodel]=ndimf.makesimspaces(rsadict, item2emomapping, orderedemos, similarity='euclidean') #model similarity space (from input space)\n",
      "        for flag in ['item']:\n",
      "            allrsasimspaces[\"%s_confs_%s\" %(chosenmodel,flag)]=ndimf.makeconfRDM(flag, chosenmodel, confmatsaves) #specialized\n",
      "    allrsasimspaces['NDEconfmat_raw']=ndimf.makeNDEconfmat(NDE_output['cprop'], orderedemos)\n",
      "    explicitdict=ndimf.createfeaturespace(allexplicitsdf, ['item']+[col for col in allexplicitsdf.columns if col not in cfg.fixedcols]) \n",
      "    ndimf.quicksave(inputspaces, os.path.join(rootdir, 'similarityspaces', 'RSAmatrixinfo.pkl'))\n",
      "    ndimf.quicksave(allrsasimspaces, os.path.join(rootdir, 'similarityspaces', 'comparisonRDMs.pkl'))\n",
      "    ndimf.quicksave(explicitdict, os.path.join(rootdir, 'similarityspaces', 'NDIMexplicits.pkl'))\n",
      "    print \"created similarity spaces acros %s iterations of split-half euclidean distances\" %(rdmiterations)\n",
      "else:\n",
      "    inputspaces=ndimf.quickload(os.path.join(rootdir, 'similarityspaces', 'RSAmatrixinfo.pkl'))\n",
      "    allrsasimspaces=ndimf.quickload(os.path.join(rootdir, 'similarityspaces', 'comparisonRDMs.pkl'))\n",
      "    explicitdict=ndimf.quickload(os.path.join(rootdir, 'similarityspaces', 'NDIMexplicits.pkl'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "created similarity spaces acros 1000 iterations of split-half euclidean distances\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Other dimensionality reduction approaches"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "avgs=ndimdf.groupby('item')[appraisals].mean()#ndimdf.groupby('emo')[appraisals].mean()\n",
      "dimlabels,dimmatrix,axislabels=avgs.columns, avgs.values.T, avgs.index"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#use affinity propogation to identify clusters in the dimensions\n",
      "%autoreload\n",
      "dimclusters, cluster2realmapping=ndimf.affinitypropcluster(dimmatrix, dimlabels, axislabels, dim1=0, dim2=1)\n",
      "print cluster2realmapping"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#kmeans\n",
      "%autoreload\n",
      "dimk=ndimf.optimalKclusters1(dimmatrix)\n",
      "if dimk<1:\n",
      "    dimk=1\n",
      "clusterdict, dimclusterresults=ndimf.kmeansclustering(dimmatrix, dimlabels, axislabels, numclust=dimk)\n",
      "print clusterdict"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#hierarchical clustering\n",
      "%autoreload\n",
      "hierarchy=ndimf.hierarchicalcluster(dimmatrix, dimlabels, similarity='euclidean', colorthresh='default')\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "%autoreload\n",
      "#PCA\n",
      "pcaobsscores, pcadimloadings, pcarecoveredinput, pcavarexplained=ndimf.reduction(dimmatrix.T, reductiontype='PCA')\n",
      "pcaobsscores, pcadimloadings, pcarecoveredinput, pcavarexplained=ndimf.reduction(dimmatrix.T, ncomp=3, reductiontype='PCA')#set this based on first screen plot\n",
      "vizfuncs.plotinputincomponentspace(pcaobsscores, axislabels, item2emomapping, plotlabels=False)\n",
      "vizfuncs.plotdimensionsincomponentspace(pcadimloadings, dimlabels, mapping=None, componentindices=[0, 1, 2], title=None)\n",
      "ndimf.printhighlowloaders(pcadimloadings, pcavarexplained, dimlabels)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#ica\n",
      "%autoreload\n",
      "icaobsscores, icadimloadings, icarecoveredinput, icavarexplained=ndimf.reduction(dimmatrix.T, reductiontype='ICA')\n",
      "icaobsscores, icadimloadings, icarecoveredinput, icavarexplained=ndimf.reduction(dimmatrix.T, ncomp=8, reductiontype='ICA')#set this based on first screen plot\n",
      "vizfuncs.plotinputincomponentspace(icaobsscores, axislabels, item2emomapping, plotlabels=False)\n",
      "vizfuncs.plotdimensionsincomponentspace(icadimloadings, dimlabels, mapping=None, componentindices=[0, 1, 2], title=None)\n",
      "ndimf.printhighlowloaders(icadimloadings, dimlabels)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#compare ICA and PCA\n",
      "%autoreload\n",
      "f,axes=plt.subplots(1,2, figsize=[10,4])\n",
      "vizfuncs.plotcomponentcorrs(pcaobsscores, ax=axes[0], label='pca')\n",
      "vizfuncs.plotcomponentcorrs(icaobsscores, ax=axes[1], label='ica')"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "%autoreload\n",
      "vizfuncs.plotcompsininputspace(dimmatrix.T, axislabels, pcadimloadings, pcavarexplained, item2emomapping, dimindices=[0, 1, 2], componentindices=[0, 1], constant=100)\n",
      "vizfuncs.plotcompsininputspace(dimmatrix.T, axislabels, icadimloadings, pcavarexplained, item2emomapping, dimindices=[0, 1, 2], componentindices=[0, 1], constant=10000)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 386
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "%autoreload\n",
      "#non parallel\n",
      "if modeling:\n",
      "    for chosenmodel in [m for m in modeltypes.keys() if m !='37dimASD']:\n",
      "        #create df that averages responses for individual items\n",
      "        modeldata,features=modeltypes[chosenmodel][0],modeltypes[chosenmodel][1]\n",
      "        #### model without generalization\n",
      "        allresults['%s_result_None' %(chosenmodel)]=ndimf.classificationwrapper(modeldata,allresults, chosenmodel, inputs['labelcol'], features, None, inputs['iterations'], orderedemos, inputs['folds'], rootdir)\n",
      "        #### model with generalization across items\n",
      "        allresults['%s_result_item' %(chosenmodel)]=ndimf.classificationwrapper(modeldata,allresults, chosenmodel, inputs['labelcol'], features, 'item', inputs['iterations'], orderedemos, inputs['folds'], rootdir)\n",
      "        confmatsaves['%s_confs_item' %(chosenmodel)]=ndimf.makeconfdict(allresults['%s_result_item' %(chosenmodel)], orderedemos)\n",
      "    ndimf.quicksave(confmatsaves, os.path.join(rootdir,'similarityspaces','NDIMsvmerrors.pkl'))\n",
      "    ndimf.quicksave(allresults, os.path.join(rootdir,'results','NDIMRESULTS.pkl'))\n",
      "else:\n",
      "    confmatsaves=ndimf.quickload(os.path.join(rootdir,'similarityspaces','NDIMsvmerrors.pkl'))\n",
      "    allresults=ndimf.quickload(os.path.join(rootdir,'results','NDIMRESULTS.pkl'))\n",
      "    print \"loaded allresults and confmatsaves\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}